{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63749483-c536-4d81-8ccf-b4ac33306265",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104e91a8-89c1-4d10-9101-2c4b4b3a521f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the Chrome WebDriver\n",
    "driver = webdriver.Chrome()\n",
    "\n",
    "# Search query for Amazon\n",
    "query = \"laptop\"\n",
    "\n",
    "# File counter for saving HTML files\n",
    "fileno = 0\n",
    "\n",
    "# Loop through the first 4 pages of search results\n",
    "for i in range(1, 5):\n",
    "    # Construct and open the Amazon search results URL for the current page\n",
    "    driver.get(f\"https://www.amazon.com/s?k={query}&page={i}&crid=5XDWP0KYCLGQ&sprefix=laptop%2Caps%2C408&ref=nb_sb_ss_ts-doa-p_1_6\")\n",
    "    \n",
    "    # Find all product elements on the page using the class name\n",
    "    elems = driver.find_elements(By.CLASS_NAME, \"puis-card-container\")\n",
    "    \n",
    "    # Print the number of items found on the page\n",
    "    print(f\"{len(elems)} items found on page {i}\")\n",
    "\n",
    "    # Iterate over each product element\n",
    "    for elem in elems:\n",
    "        # Get the outer HTML of the element\n",
    "        d = elem.get_attribute(\"outerHTML\")\n",
    "        \n",
    "        # Save the HTML content to a file in the \"data\" directory\n",
    "        with open(f\"data/{query}_{fileno}.html\", \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(d)\n",
    "        \n",
    "        # Increment file number for unique filenames\n",
    "        fileno += 1\n",
    "\n",
    "    # Pause for 2 seconds before loading the next page (to avoid being blocked)\n",
    "    time.sleep(2)\n",
    "\n",
    "# Close the browser after scraping is done\n",
    "driver.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e721e81-d434-4283-91c1-163307fb359b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary to store extracted product details\n",
    "d = {'title': [], 'price': [], 'link': []}\n",
    "\n",
    "# Loop through all files in the \"data\" directory\n",
    "for file in os.listdir(\"data\"):\n",
    "    try: \n",
    "        # Open and read the HTML file\n",
    "        with open(f\"data/{file}\", \"r\", encoding=\"utf-8\") as f:\n",
    "            html_doc = f.read()\n",
    "\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(html_doc, 'html.parser')\n",
    "\n",
    "        # Extract the product title (assumed to be inside an <h2> tag)\n",
    "        t = soup.find(\"h2\")\n",
    "        title = t.get_text()\n",
    "    \n",
    "        # Extract the product price (assumed to be inside a <span> tag with class \"a-color-base\")\n",
    "        p = soup.find(\"span\", attrs={\"class\": 'a-color-base'})\n",
    "        price = p.get_text()\n",
    "        \n",
    "        # Extract the product link (finds the parent <a> tag of the title)\n",
    "        l = t.find_parent('a')\n",
    "        link = \"https://www.amazon.com\" + l.get('href')\n",
    "\n",
    "        # Append extracted details to the dictionary\n",
    "        d['title'].append(title)\n",
    "        d['price'].append(price)\n",
    "        d['link'].append(link)\n",
    "        \n",
    "        # Uncomment below if you want to see the extracted data while running\n",
    "        # print(title, link, price)\n",
    "\n",
    "    except Exception as e:\n",
    "        # Print any errors encountered while processing a file\n",
    "        print(e)\n",
    "\n",
    "# Convert the dictionary into a DataFrame\n",
    "df = pd.DataFrame(data=d)\n",
    "\n",
    "# Save the extracted data into a CSV file\n",
    "df.to_csv(\"data.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d5b065-f5c0-47e9-80c9-3323b4de5548",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40baeb60-c281-4fb5-bc49-f295b66eb68c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
